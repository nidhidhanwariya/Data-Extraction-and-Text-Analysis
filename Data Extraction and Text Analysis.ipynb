{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ROPChgKKgu2"
   },
   "source": [
    "Data Extraction and Text Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ggr8jzpIKt5w"
   },
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jFmnAgKSKx4b",
    "outputId": "17130dc4-db2e-40a1-e3e5-84e48b53073c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting xlsxwriter\n",
      "  Downloading XlsxWriter-3.0.3-py3-none-any.whl (149 kB)\n",
      "\u001b[K     |████████████████████████████████| 149 kB 5.1 MB/s \n",
      "\u001b[?25hInstalling collected packages: xlsxwriter\n",
      "Successfully installed xlsxwriter-3.0.3\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting textstat\n",
      "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 5.1 MB/s \n",
      "\u001b[?25hCollecting pyphen\n",
      "  Downloading pyphen-0.12.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 37.1 MB/s \n",
      "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
      "Successfully installed pyphen-0.12.0 textstat-0.7.3\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting excel2json-3\n",
      "  Downloading excel2json_3-0.1.6-py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: xlrd in /usr/local/lib/python3.7/dist-packages (from excel2json-3) (1.1.0)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (from excel2json-3) (3.0.10)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from excel2json-3) (2.23.0)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl->excel2json-3) (1.1.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->excel2json-3) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->excel2json-3) (2022.5.18.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->excel2json-3) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->excel2json-3) (1.24.3)\n",
      "Installing collected packages: excel2json-3\n",
      "Successfully installed excel2json-3-0.1.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "!pip install xlsxwriter\n",
    "!pip install textstat\n",
    "!pip install excel2json-3\n",
    "\n",
    "import plotly.graph_objs\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import re\n",
    "import urllib\n",
    "import textstat\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from textblob import TextBlob\n",
    "\n",
    "import excel2json\n",
    "import xlsxwriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPGzDlEYNgiU"
   },
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jFSXv8IqNATs"
   },
   "outputs": [],
   "source": [
    "excel2json.convert_from_file('Input.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJ0qNVoRLNUF"
   },
   "source": [
    "Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20iq4osuMdng"
   },
   "source": [
    " A Function which loops over the URLs which are in the dictionary which is used as a paramter for the fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "60lXaqmCMj2m"
   },
   "outputs": [],
   "source": [
    "def main(y):\n",
    "  for k in y:\n",
    "    url = str(y[k])\n",
    "    headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"}\n",
    "    r = requests.get(url=url, headers=headers)\n",
    "    soup = BeautifulSoup(r.content, 'html5lib')\n",
    "\n",
    "    #Extracts title from the url\n",
    "    title=soup.find('h1',class_=\"entry-title\")\n",
    "    title=title.text.replace('\\n',\" \")\n",
    "\n",
    "    #Extracts content from url\n",
    "    content=soup.findAll(attrs={'class':'td-post-content'})\n",
    "    content=content[0].text.replace('\\n',\" \")\n",
    "    content = content.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    #Converting content into tokens\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text_tokens = word_tokenize(content)\n",
    "\n",
    "    #Removing stopwords from the tokens\n",
    "    my_stop_words = stopwords.words('english')\n",
    "    my_stop_words.append('the')\n",
    "    no_stop_tokens = [word for word in text_tokens if not word in my_stop_words]\n",
    "\n",
    "    #Checking for the positive words using positive-words.txt\n",
    "    with open(\"positive-words.txt\",'r') as pos_word:\n",
    "     pos_words = pos_word.read().split(\"\\n\")\n",
    "     pos_words = pos_words[5:]\n",
    "\n",
    "    pos_count = \" \".join([w for w in no_stop_tokens if w in pos_words])\n",
    "    pos_count = pos_count.split(\" \")\n",
    "    Positive_score=len(pos_count) \n",
    "    \n",
    "    #Checking for the negative words using negative-words.txt\n",
    "    with open(\"negative-words.txt\",\"r\",encoding = \"ISO-8859-1\") as neg:\n",
    "      negwords = neg.read().split(\"\\n\")\n",
    "      negwords = negwords[36:]\n",
    "\n",
    "    neg_count = \" \".join ([w for w in no_stop_tokens if w in negwords])\n",
    "    neg_count=neg_count.split(\" \")\n",
    "    Negative_score=len(neg_count)\n",
    "    \n",
    "    filter_content = ' '.join(no_stop_tokens)\n",
    "\n",
    "    #Calculating Polarity Score and Subjectivity Score\n",
    "    sentiment=TextBlob(content).sentiment\n",
    "\n",
    "    #Average Sentence length\n",
    "    AVG_SENTENCE_LENGTH = len(content.replace(' ',''))/len(re.split(r'[?!.]', content))\n",
    "\n",
    "    #FOG Index\n",
    "    FOG_index=(textstat.gunning_fog(content))\n",
    "\n",
    "    #Average Number of words per Sentence\n",
    "    AVG_NUMBER_OF_WORDS_PER_SENTENCE = [len(l.split()) for l in re.split(r'[?!.]', content) if l.strip()]\n",
    "    AVG_NUMBER_OF_WORDS_PER_SENTENCE=sum(AVG_NUMBER_OF_WORDS_PER_SENTENCE)/len(AVG_NUMBER_OF_WORDS_PER_SENTENCE)\n",
    "\n",
    "    #Complex word Count\n",
    "    def syllable_count(word):\n",
    "      count = 0\n",
    "      vowels = \"AEIOUYaeiouy\"\n",
    "      if word[0] in vowels:\n",
    "          count += 1\n",
    "      for index in range(1, len(word)):\n",
    "          if word[index] in vowels and word[index - 1] not in vowels:\n",
    "              count += 1\n",
    "              if word.endswith(\"es\"or \"ed\"):\n",
    "                  count -= 1\n",
    "      if count == 0:\n",
    "          count += 1\n",
    "      return count\n",
    "\n",
    "    COMPLEX_WORDS=syllable_count(content)\n",
    "\n",
    "    #Word Counts\n",
    "    Words_counts=len(content)\n",
    "\n",
    "    #Percentage of Complex words\n",
    "    pcw=(COMPLEX_WORDS/Words_counts)*100\n",
    "\n",
    "    #Average Word Length\n",
    "    Average_Word_Length=len(content.replace(' ',''))/len(content.split())\n",
    "\n",
    "    #Syllable per word\n",
    "    word=content.replace(' ','')\n",
    "    syllable_count=0\n",
    "    for w in word:\n",
    "          if(w=='a' or w=='e' or w=='i' or w=='o' or w=='y' or w=='u' or w=='A' or w=='E' or w=='I' or w=='O' or w=='U' or w=='Y'):\n",
    "                syllable_count=syllable_count+1\n",
    "    spw = syllable_count/len(content.split())\n",
    "\n",
    "    #Personal Pronouns\n",
    "    def ProperNounExtractor(text):\n",
    "      count = 0\n",
    "      sentences = nltk.sent_tokenize(text)\n",
    "      for sentence in sentences:\n",
    "          words = nltk.word_tokenize(sentence)\n",
    "          tagged = nltk.pos_tag(words)\n",
    "          for (word, tag) in tagged:\n",
    "              if tag == 'PRP':\n",
    "                count+=1\n",
    "\n",
    "      return(count)\n",
    "    Personal_Pronouns=ProperNounExtractor(content)\n",
    "    \n",
    "    #Adding individual elements of a row in dataframe\n",
    "    list =[url,title,content,Positive_score,Negative_score,sentiment.polarity,sentiment.subjectivity, AVG_SENTENCE_LENGTH, FOG_index, AVG_NUMBER_OF_WORDS_PER_SENTENCE, COMPLEX_WORDS, Words_counts, pcw, Average_Word_Length, spw, Personal_Pronouns]\n",
    "\n",
    "    #Converting str to int\n",
    "    z= int(k)\n",
    "\n",
    "    #Adding one list ata atime to the dataframe\n",
    "    df.loc[z] = list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qp9zTRhDMKSP"
   },
   "outputs": [],
   "source": [
    "#Converting excel to json\n",
    "excel_data_df = pd.read_excel('Input.xlsx', sheet_name='Sheet1')\n",
    "json_str = excel_data_df.to_json()\n",
    "\n",
    "#Converting json to dictionary via loads method\n",
    "x = json.loads(json_str)\n",
    "\n",
    "#To fetch only URLS\n",
    "y = x['URL']\n",
    "\n",
    "#Defining the dataframe\n",
    "df=pd.DataFrame(columns=['url','title','content','Positive_Score','Negative_Score', 'Polarity', 'Subjectivity', 'AVG_SENTENCE_LENGTH', 'FOG_index', 'AVG_NUMBER_OF_WORDS_PER_SENTENCE', 'COMPLEX_WORDS', 'Words_counts', 'Percentage_of_complex_words', 'Average_Word_Length', 'Syllable_per_word', 'Personal pronouns'])\n",
    "\n",
    "#Calling the main function\n",
    "main(y)\n",
    "\n",
    "#Generation Output file of the dataframe\n",
    "with pd.ExcelWriter('output.xlsx') as writer:  \n",
    "\n",
    "    df.to_excel(writer, sheet_name='Sheet1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aT9whdfYL92J"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "BLACKCOFFER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
